{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##%tensorboard --logdir ../output_1500/train\n",
    "!pip install seaborn\n",
    "!pip install pandas\n",
    "!pip install praat-textgrids\n",
    "!pip install pydub\n",
    "!pip install pyrle\n",
    "!pip install ipywidgets\n",
    "!apt-get install -y ffmpeg\n",
    "!jupyter labextension install @jupyter-widgets/jupyterlab-manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastPitch: Voice Modification with Pre-defined Pitch Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [FastPitch](https://arxiv.org/abs/2006.06873) model is based on the [FastSpeech](https://arxiv.org/abs/1905.09263) model. Similarly to [FastSpeech2](https://arxiv.org/abs/2006.04558), which has been developed concurrently, it learns to predict the pitch contour and conditions the generation on such contour.\n",
    "\n",
    "The simple mechanism of predicting the pitch on grapheme-level (rather than frame-level, as FastSpeech2 does) allows to easily alter the pitch during synthesis. FastPitch can thus change the perceived emotional state of the speaker, or slightly emphasise certain lexical units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the notebook inside the container. By default the container forwards port `8888`.\n",
    "```\n",
    "bash scripts/docker/interactive.sh\n",
    "\n",
    "# inside the container\n",
    "cd notebooks\n",
    "jupyter notebook --ip='*' --port=8888\n",
    "```\n",
    "Please refer the Requirement section in `README.md` for more details and running outside the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "assert os.getcwd().split('/')[-1] == 'notebooks'\n",
    "\n",
    "import IPython\n",
    "from IPython.display import display, Markdown, Audio\n",
    "import requests\n",
    "import torch\n",
    "import numpy as np\n",
    "import io\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.io.wavfile import read, write\n",
    "import warnings\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "import tempfile\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [30, 15]\n",
    "from ipywidgets import HBox, Label, VBox\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import Audio, display\n",
    "import parselmouth\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate audio samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a FastPitch model from scrath takes 3 to 27 hours depending on the type and number of GPUs, performance numbers can be found in Section \"Training performance results\" in `README.md`. Therefore, to save the time of running this notebook, we recommend to download the pretrained FastPitch checkpoints on NGC for inference.\n",
    "\n",
    "You can find FP32 checkpoint at [NGC](https://ngc.nvidia.com/catalog/models/nvidia:fastpitch_pyt_fp32_ckpt_v1/files) , and AMP (Automatic Mixed Precision) checkpoint at [NGC](https://ngc.nvidia.com/catalog/models/nvidia:fastpitch_pyt_amp_ckpt_v1/files).\n",
    "\n",
    "To synthesize audio, you will need a WaveGlow model, which generates waveforms based on mel-spectrograms generated by FastPitch.You can download a pre-trained WaveGlow AMP model at [NGC](https://ngc.nvidia.com/catalog/models/nvidia:waveglow256pyt_fp16)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! mkdir -p output\n",
    "# ! MODEL_DIR='../pretrained_models' ../scripts/download_fastpitch.sh\n",
    "# ! MODEL_DIR='../pretrained_models' ../scripts/download_waveglow.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can perform inference using the respective checkpoints that are passed as `--fastpitch` and `--waveglow` arguments. Next, you will use FastPitch model to generate audio samples for input text, including the basic version and the variations i npace, fade out, and pitch transforms, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store paths in aux variables\n",
    "fastp0 = '../output/FastPitch_checkpoint_100.pt'\n",
    "fastp1500 = '../output/FastPitch_checkpoint_1500.pt'\n",
    "fastp3000 = '../output/FastPitch_checkpoint_3000.pt'\n",
    "waveg = '../pretrained_models/waveglow/nvidia_waveglow256pyt_fp16.pt'\n",
    "flags0 = f'--cuda --fastpitch {fastp0} --waveglow {waveg} --wn-channels 256'\n",
    "flags1500 = f'--cuda --fastpitch {fastp1500} --waveglow {waveg} --wn-channels 256'\n",
    "flags3000 = f'--cuda --fastpitch {fastp3000} --waveglow {waveg} --wn-channels 256'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Basic speech synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to create an input file with some text, or just input the text in the below cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile text.txt\n",
    "I felt somehow it would have been an easier job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "tts_node = \"https://tts.test.vocacloud.net\"\n",
    "#tts_node = \"http://172.36.72.216:5000\" #172.20.128.3\n",
    "#tts_node = \"http://172.20.128.2:5000\" \n",
    "\n",
    "def get_temp_filename(mydir=\".\",prefix=\"temp_\", suffix=\"\"):\n",
    "    \n",
    "    \"\"\" return a string in the form of temp_X, where X is a large integer \"\"\"\n",
    "    file = tempfile.mkstemp(suffix=suffix, prefix=prefix, dir=mydir) \n",
    "    os.close(file[0])\n",
    "    return file[1] \n",
    "\n",
    "def generate_kwargs_for_voca(text,what):\n",
    "    headers = {'Authorization': 'Bearer a2f4cd32-f5f6-46fa-952a-b27697382b10',\n",
    "               'Content-Type': 'application/json; version=1'}\n",
    "    kwargs = {}\n",
    "    kwargs['headers'] = headers\n",
    "    json_data = {\"sentence\": text, \"volume\": 1, \"speed\": 1, \"sampleRate\": 22050, \"reply_fields\": what}\n",
    "    return json_data, kwargs\n",
    "\n",
    "def getDetailsFromTTS(text,what,url):\n",
    "    sentence = text\n",
    "    json_data, kwargs = generate_kwargs_for_voca(sentence, what)\n",
    "    rawResponse = requests.post(url, json=json_data, stream=True, verify=False, **kwargs)\n",
    "    reply = torch.load(io.BytesIO(rawResponse.content))\n",
    "    return reply\n",
    "\n",
    "def get_current_jenny(input):\n",
    "    with open(input,\"r\") as f:\n",
    "        text = f.readline()\n",
    "    url = tts_node+\"/tts/generate\"\n",
    "    rep = getDetailsFromTTS(text,['mel', 'align_logist', 'frame_length', 'sample_rate',\"signal\",\"align_text\"],url)\n",
    "    return rep[\"signal\"].numpy(),int(rep[\"sample_rate\"]),rep[\"mel\"].numpy(),rep[\"align_logist\"].numpy(),rep[\"align_text\"].numpy()\n",
    "    \n",
    "# out,sr,mel,align, align_text = get_current_jenny(\"text.txt\")    \n",
    "# write(\"/tmp/dummy.wav\", sr,out)\n",
    "# plt.imshow(mel)\n",
    "# plt.show()\n",
    "# plt.imshow(align)\n",
    "# plt.show()\n",
    "\n",
    "# a = get_temp_filename(\"/tmp\",\"temp_\",'.wav')\n",
    "# print(a)\n",
    "\n",
    "text = \"hi there\"\n",
    "what = ['mel', 'align_logist', 'frame_length', 'sample_rate',\"signal\",\"align_text\"]\n",
    "url = \"https://tts.test.vocacloud.net\"+\"/tts/generate\"\n",
    "json_data, kwargs = generate_kwargs_for_voca(text, what)\n",
    "rawResponse = requests.post(url, json=json_data, stream=True, verify=False, **kwargs)\n",
    "rawResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "what"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Run the script below to generate audio from the input text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences = {}\n",
    "with open(\"../Jenny/metadata.csv\",\"r\") as f:\n",
    "    lines = f.readlines()\n",
    "for line in lines:\n",
    "    s = line.split(\"|\")\n",
    "    sentences[s[1]] = s[0]\n",
    "    \n",
    "def find_in_train_set(text):\n",
    "    if text in sentences:\n",
    "        return sentences[text]\n",
    "    else:\n",
    "        None\n",
    "        \n",
    "for s in sentences.keys():\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compare_sound(text,include_tacotron):\n",
    "    # gen temps\n",
    "    filename = get_temp_filename(\"/tmp\",\"temp_\",\".txt\")\n",
    "    filename2 = get_temp_filename(\"/tmp\",\"temp_\",\".wav\")\n",
    "\n",
    "    # write file\n",
    "    with open(filename,\"w\") as f:\n",
    "        f.write(text)\n",
    "        \n",
    "    display(Markdown(\"Original \"+find_in_train_set(text)))\n",
    "    train_id = find_in_train_set(text)\n",
    "    if train_id:\n",
    "        train_file = \"../Jenny/wavs/{}.wav\".format(train_id)\n",
    "        display(Audio(train_file))\n",
    "    else:\n",
    "        display(Markdown(\"Not in train set\"))\n",
    "\n",
    "    mel = None\n",
    "    align = None\n",
    "    align_text = text\n",
    "    if include_tacotron:\n",
    "        display(Markdown(\"Tacotron2\"))\n",
    "        # get current\n",
    "        out,sr,mel,align,align_text = get_current_jenny(filename)    \n",
    "        write(filename2, sr,out)\n",
    "        display(Audio(filename2))\n",
    "    \n",
    "    display(Markdown(\"fastPitch 0\"))\n",
    "    !python3 ../inference.py {flags0} -i {filename} -o output/0 --speaker 0 --n-speakers 2 >/dev/null\n",
    "    display(Audio(\"output/0/audio_0.wav\"))\n",
    "    \n",
    "    #display(Markdown(\"fastPitch 1500\"))\n",
    "    #!python3 ../inference.py {flags1500} -i {filename} -o output/1500 --speaker 0 --n-speakers 2 >/dev/null\n",
    "    #IPython.display.display(IPython.display.Audio(\"output/1500/audio_0.wav\"))\n",
    "    \n",
    "    display(Markdown(\"fastPitch 3000\"))\n",
    "    !python3 ../inference.py {flags3000} -i {filename} -o output/3000 --speaker 0 --n-speakers 2 >/dev/null\n",
    "    display(Audio(\"output/3000/audio_0.wav\"))\n",
    "    return train_id, mel,filename2, align, align_text\n",
    "    \n",
    "#compare_sound(\"There alone could lovers see each other and communicate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "id,mel,tc2_filename,align2,align_text2 = compare_sound(\"But it involves a happiness that will last throughout our lives, will it not?\",False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "text = \"But it involves a happiness that will last throughout our lives, will it not?\"\n",
    "display(Markdown(\"Tacotron2\"))\n",
    "fn = get_temp_filename(\"/tmp\",\"temp_\",\".txt\")\n",
    "filename = get_temp_filename(\"/tmp\",\"temp_\",\".wav\")\n",
    "with open(fn,\"w\") as f:\n",
    "    f.write(text)\n",
    "\n",
    "out,sr,mel,align,align_text = get_current_jenny(fn)    \n",
    "write(filename, sr,out)\n",
    "display(Audio(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# now collect the examples and get their pitch\n",
    "def get_pitch(wav):\n",
    "    snd = parselmouth.Sound(wav)\n",
    "    pitch = snd.to_pitch().selected_array['frequency'] #time_step=snd.duration / (mel_len + 3)\n",
    "    return pitch\n",
    "\n",
    "# p1 = get_pitch(\"../Jenny/wavs/05363.wav\")\n",
    "# print(len(p1))\n",
    "# p2 = get_pitch(\"output/0/audio_0.wav\")\n",
    "# print(len(p2))\n",
    "# p3 = get_pitch(\"output/3000/audio_0.wav\")\n",
    "# print(len(p3))\n",
    "# p4 = get_pitch(filename2)\n",
    "# print(len(p4))\n",
    "# t = min(len(p1),len(p2),len(p3))\n",
    "# plt.plot(np.column_stack((p1[0:t],p2[0:t],p3[0:t])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's do force aligner\n",
    "def mfa_align(text,in_wav_file,start_sec = 0,duration_sec=None):\n",
    "    in_folder = \"/tmp/test\"\n",
    "    with open(in_folder+\"/0.txt\",\"w\") as f:\n",
    "        f.write(text)\n",
    "        \n",
    "    #cat ../../Jenny/metadata.csv | awk -F '|' '$1==05363 {print $2}' > /tmp/test/0.txt\n",
    "    out_folder = \"/tmp/test2\"\n",
    "    wav_file = in_folder + \"/0.wav\"\n",
    "    !rm {wav_file}\n",
    "    trim = \"\"\n",
    "    if duration_sec:\n",
    "        trim = f\"trim {start_sec} {duration_sec}\"\n",
    "    !sox {in_wav_file} -r 22050 -b 16 {wav_file} {trim} \n",
    "    !/workspace/fastpitch/MFA/montreal-forced-aligner/bin/mfa_align {in_folder} /workspace/fastpitch/MFA/montreal-forced-aligner/librispeech-lexicon.txt /workspace/fastpitch/MFA/montreal-forced-aligner/pretrained_models/english.zip {out_folder}\n",
    "    \n",
    "    return out_folder+\"/test/0.TextGrid\",wav_file\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# snd = parselmouth.Sound(\"/tmp/test/0.wav\")\n",
    "# sns.set() # Use seaborn's default style to make attractive graphs\n",
    "# plt.rcParams['figure.dpi'] = 100 # Show nicely large images in this notebook\n",
    "# snd_part = snd.extract_part(from_time=0.9, preserve_times=True)\n",
    "# plt.figure()\n",
    "# plt.plot(snd_part.xs(), snd_part.values.T, linewidth=0.5)\n",
    "# plt.xlim([snd_part.xmin, snd_part.xmax])\n",
    "# plt.xlabel(\"time [s]\")\n",
    "# plt.ylabel(\"amplitude\")\n",
    "# plt.show()\n",
    "# def draw_spectrogram(spectrogram, dynamic_range=70):\n",
    "#     X, Y = spectrogram.x_grid(), spectrogram.y_grid()\n",
    "#     sg_db = 10 * np.log10(spectrogram.values)\n",
    "#     plt.pcolormesh(X, Y, sg_db, vmin=sg_db.max() - dynamic_range, cmap='afmhot')\n",
    "#     plt.ylim([spectrogram.ymin, spectrogram.ymax])\n",
    "#     plt.xlabel(\"time [s]\")\n",
    "#     plt.ylabel(\"frequency [Hz]\")\n",
    "\n",
    "# def draw_intensity(intensity):\n",
    "#     plt.plot(intensity.xs(), intensity.values.T, linewidth=3, color='w')\n",
    "#     plt.plot(intensity.xs(), intensity.values.T, linewidth=1)\n",
    "#     plt.grid(False)\n",
    "#     plt.ylim(0)\n",
    "#     plt.ylabel(\"intensity [dB]\")\n",
    "# intensity = snd.to_intensity()\n",
    "# spectrogram = snd.to_spectrogram()\n",
    "# plt.figure()\n",
    "# draw_spectrogram(spectrogram)\n",
    "# plt.twinx()\n",
    "# draw_intensity(intensity)\n",
    "# plt.xlim([snd.xmin, snd.xmax])\n",
    "# plt.show()\n",
    "\n",
    "# def draw_pitch(pitch):\n",
    "#     # Extract selected pitch contour, and\n",
    "#     # replace unvoiced samples by NaN to not plot\n",
    "#     pitch_values = pitch.selected_array['frequency']\n",
    "#     pitch_values[pitch_values==0] = np.nan\n",
    "#     plt.plot(pitch.xs(), pitch_values, 'o', markersize=5, color='w')\n",
    "#     plt.plot(pitch.xs(), pitch_values, 'o', markersize=2)\n",
    "#     plt.grid(False)\n",
    "#     plt.ylim(0, pitch.ceiling)\n",
    "#     plt.ylabel(\"fundamental frequency [Hz]\")\n",
    "    \n",
    "# pitch = snd.to_pitch()\n",
    "# pre_emphasized_snd = snd.copy()\n",
    "# pre_emphasized_snd.pre_emphasize()\n",
    "# spectrogram = pre_emphasized_snd.to_spectrogram(window_length=0.03, maximum_frequency=8000)\n",
    "# plt.figure()\n",
    "# draw_spectrogram(spectrogram)\n",
    "# plt.twinx()\n",
    "# draw_pitch(pitch)\n",
    "# plt.xlim([snd.xmin, snd.xmax])\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import textgrids\n",
    "import pandas as pd\n",
    "\n",
    "in_folder = \"/tmp/test\"\n",
    "out_folder = \"/tmp/test2\"\n",
    "gridFile = out_folder+\"/test/\"+\"0.TextGrid\"\n",
    "wavFile = in_folder+\"/0.wav\"\n",
    "\n",
    "def get_pitch_for_grid(gridFile,wavFile,res):\n",
    "    snd = parselmouth.Sound(wavFile)\n",
    "    grid = textgrids.TextGrid(gridFile)\n",
    "    pitch = snd.to_pitch()\n",
    "    freq = pitch.selected_array['frequency']\n",
    "    print(len(freq))\n",
    "    freq[freq==0] = np.nan\n",
    "    for i,t in enumerate(grid['phones']):\n",
    "        m = np.nanmean(freq[np.array(np.logical_and(t.xmax>pitch.xs() , t.xmin<pitch.xs()))])\n",
    "        s = np.nanstd(freq[np.array(np.logical_and(t.xmax>pitch.xs() , t.xmin<pitch.xs()))])\n",
    "        res = res.append(pd.DataFrame({\"file\":wavFile,\"id\":i,\"text\":t.text,\"mean\":m,\"std\":s,\"min\":t.xmin,\"max\":t.xmax},index=[0]))\n",
    "    return res\n",
    "\n",
    "!mkdir -p {in_folder}\n",
    "!mkdir -p {out_folder}\n",
    "# out = {}\n",
    "# res = pd.DataFrame()\n",
    "# for file in [\"../Jenny/wavs/05363.wav\",filename2,\"output/3000/audio_0.wav\"]:\n",
    "#     tg,wf = mfa_align(\"But it involves a happiness that will last throughout our lives, will it not?\",file)\n",
    "#     res = get_pitch_for_grid(tg,wf,res)\n",
    "# res    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import requests\n",
    "import torch\n",
    "import numpy as np\n",
    "from pyrle import Rle as rle\n",
    "import io\n",
    "from scipy.io.wavfile import write\n",
    "lables = [    ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', ':', ';', '?', 'A', 'B',\n",
    "              'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P',\n",
    "              'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', 'a', 'b',\n",
    "              'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p',\n",
    "              'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z' ,'@']\n",
    "maping = {}\n",
    "demapping = {}\n",
    "cc = 1\n",
    "for c in lables:\n",
    "    maping[c] = cc\n",
    "    demapping[cc] = c\n",
    "    cc+=1\n",
    "    \n",
    "def get_aligment(logits, padded_text,frame_length):\n",
    "    \"\"\"\n",
    "    gets the alignment of the padded text using the logits of the generated speech.\n",
    "    :param logits: matrix of character probabilities for each ts\n",
    "    :param padded_text: encoded padded text\n",
    "    :return:list of lists - each element delimits the frames the word was uttered\n",
    "    \"\"\"\n",
    "    best_prob_per_frame = np.argmax(logits, axis=0).tolist()\n",
    "    best_chars = [padded_text[frame] for frame in best_prob_per_frame]\n",
    "    mid_sentence = int(logits.shape[1] / 2)\n",
    "    # get first non-padding char\n",
    "    first_non_pad_char = np.min(np.where(padded_text != maping['@']))\n",
    "    start_frame = np.argmax(logits[first_non_pad_char, :mid_sentence])\n",
    "    # get last non padding char\n",
    "    last_non_pad_char = np.max(np.where(padded_text != maping['@']))\n",
    "    end_frame = mid_sentence+np.argmax(logits[last_non_pad_char, mid_sentence:])\n",
    "    #get RLE of best_chars - removing repeating chars\n",
    "    rle_all = rle(best_chars[start_frame:end_frame])\n",
    "    rle_dur = rle_all.runs\n",
    "    rle_char = rle_all.values.astype(int)\n",
    "    # note that the beginning of the alignment is not very \"focused\" so skip to start pos\n",
    "    padded_text = padded_text[first_non_pad_char:(last_non_pad_char+1)]\n",
    "    r_pos = 0\n",
    "    frame = start_frame\n",
    "    match = -1*np.ones((len(padded_text),2))\n",
    "    #\n",
    "    # The following matches the RLE encoding and the aligned_chars map\n",
    "    #\n",
    "    for i,c in enumerate(padded_text):\n",
    "        if c == rle_char[r_pos]:\n",
    "            # there is a match\n",
    "            match[i,:] = (frame,frame+rle_dur[r_pos])\n",
    "            frame += rle_dur[r_pos]\n",
    "            r_pos += 1\n",
    "        elif rle_char[r_pos]==maping[' '] and c == rle_char[r_pos+1]:\n",
    "            # allow floating spaces in the middle of the words\n",
    "            match[i,:] = (frame+rle_dur[r_pos],frame+rle_dur[r_pos]+rle_dur[r_pos+1])\n",
    "            frame += (rle_dur[r_pos]+rle_dur[r_pos+1])\n",
    "            r_pos += 2\n",
    "        # if char was not found - it remains None\n",
    "    # fix chars that were not found\n",
    "    missing_values = np.where(match[:,0]==-1)[0]\n",
    "    if len(missing_values)>0:\n",
    "        prev_ends = match[(np.array(missing_values)-1),1]\n",
    "        match[missing_values,] = np.stack((prev_ends,prev_ends),axis=1)\n",
    "    negative_values = np.where(match[:,0]<0)[0]\n",
    "    if len(negative_values)>0:\n",
    "        prev_ends = match[(np.array(negative_values)-1),1]\n",
    "        match[negative_values,] = np.stack((prev_ends,prev_ends),axis=1)\n",
    "    # concat everything together\n",
    "    get_all = match* frame_length #np.hstack((match*frame_length,np.array([demapping[c] for c in padded_text]).reshape(match.shape[0],-1)))\n",
    "    # get the places with spaces\n",
    "    return get_all,padded_text\n",
    "\n",
    "# fl = 256/22050\n",
    "# alignment,padded_text = get_aligment(align, align_text,fl)\n",
    "\n",
    "# print(alignment.shape)\n",
    "# print(len(padded_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "# let's rerun everything....\n",
    "#\n",
    "text = \"hi there\"\n",
    "what = ['mel', 'align_logist', 'frame_length', 'sample_rate',\"signal\",\"align_text\"]\n",
    "url = \"https://tts.test.vocacloud.net\"+\"/tts/generate\"\n",
    "json_data, kwargs = generate_kwargs_for_voca(text, what)\n",
    "rawResponse = requests.post(url, json=json_data, stream=True, verify=False, **kwargs)\n",
    "rawResponse\n",
    "\n",
    "#\n",
    "# from Tacotron!\n",
    "#\n",
    "text = \"But it involves a happiness that will last throughout our lives, will it not?\"\n",
    "fn = get_temp_filename(\"/tmp\",\"temp_\",\".txt\")\n",
    "filename = get_temp_filename(\"/tmp\",\"temp_\",\".wav\")\n",
    "with open(fn,\"w\") as f:\n",
    "    f.write(text)\n",
    "out,sr,mel,align,align_text = get_current_jenny(fn)    \n",
    "write(filename, sr,out)\n",
    "fl = 256/22050\n",
    "letters_alignment,padded_text = get_aligment(align, align_text,fl)\n",
    "s = 0\n",
    "e = -1\n",
    "duration = letters_alignment[e,1]-letters_alignment[s,0]\n",
    "start = letters_alignment[s,0]\n",
    "tg,wf = mfa_align(text,filename,start,duration)\n",
    "phones = get_pitch_for_grid(tg,wf,pd.DataFrame())\n",
    "phones['min'] = phones['min'] + letters_alignment[0,0]\n",
    "phones['max'] = phones['max'] + letters_alignment[0,0]\n",
    "\n",
    "#\n",
    "# from ground_truth\n",
    "#\n",
    "train_id = find_in_train_set(text)\n",
    "train_file = \"../Jenny/wavs/{}.wav\".format(train_id)\n",
    "gt_tg,gt_wf = mfa_align(text,train_file)\n",
    "gt_phones = get_pitch_for_grid(gt_tg,gt_wf,pd.DataFrame())\n",
    "#gt_phones['min'] = gt_phones['min'] + letters_alignment[0,0]\n",
    "#gt_phones['max'] = gt_phones['max'] + letters_alignment[0,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now view all together\n",
    "def draw_phones(phones,colour,compare=None,compare_colour=\"k\"):\n",
    "    plt.hlines(phones['mean'],phones['min'],phones['max'],color=colour)\n",
    "    for i in range(phones['min'].shape[0]):\n",
    "        x = phones.loc[i,'min']\n",
    "        if np.isfinite(phones.loc[i,'mean']):\n",
    "            plt.text(float(x),phones.loc[i,'mean'],phones.loc[i,'text'],fontsize=18,color=colour)\n",
    "        if compare is not None:\n",
    "            id = phones.loc[i,'id']\n",
    "            new_h = compare['mean'][(compare['id']==id)]\n",
    "            plt.hlines(new_h,phones.loc[i,'min'],phones.loc[i,'max'],color=compare_colour)\n",
    "    \n",
    "def draw_letters(padded_text,letters_alignment,colour):\n",
    "    pt = [demapping[t] for t in padded_text.tolist()]\n",
    "    for i,t in enumerate(pt):\n",
    "        if i % 2:\n",
    "            h = 100\n",
    "        else:\n",
    "            h = 90\n",
    "        if t==\" \":\n",
    "            t = \"-\"\n",
    "        if np.isfinite(letters_alignment[i,0]) and letters_alignment[i,0]>0:\n",
    "            plt.vlines(letters_alignment[i,0],1,500,linestyle=\":\",color=\"gray\")\n",
    "            plt.vlines(letters_alignment[i,1],1,500,linestyle=\":\",color=\"gray\")\n",
    "            plt.text(letters_alignment[i,0],h,t,fontsize=18,color=colour)\n",
    "            plt.hlines(h-20,letters_alignment[i,0],letters_alignment[i,1],color=colour)\n",
    "\n",
    "phones = phones.reset_index(drop=True)\n",
    "gt_phones = gt_phones.reset_index(drop=True)\n",
    "\n",
    "draw_phones(phones,\"r\",gt_phones)\n",
    "draw_letters(padded_text,letters_alignment,\"b\")\n",
    "# plt.show()\n",
    "# draw_phones(gt_phones,\"k\")\n",
    "print(\"GT\")\n",
    "display(Audio(train_file))\n",
    "print(\"tacotron\")\n",
    "display(Audio(filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../pitch_transform.py\n",
    "import torch\n",
    "\n",
    "def pitch_transform_custom(pitch, duration):\n",
    "# pitch = pitch + 110\n",
    "#     print(\"---!\")\n",
    "#     #duration = duration * 0 + 1.5\n",
    "#     #pitch = pitch * 0 + 300\n",
    "#     print(duration)\n",
    "#     print(pitch)\n",
    "#     #pitch_offset = [0, -11.734620465350275, -28.689850792933385, 0, -44.61186523887815, -45.70688894982871, 0, -56.560140114505515, -52.77171128844202, -58.20461132565032, -105.01983787614819, -124.48284897082337, -116.23204895517284, -116.23204895517284, 0, 0, -64.71739562158871, 0, -38.5914461297364, -42.897411924509385, 0, 0, -3.3426176814817268, -12.247741709341824, 5.828423138490507, 0, 0, 0, 0, 0, 0, 295.5051434349008, 0, -13.84992685544674, -8.756527112814183, -11.744156656968755, -11.744156656968755, 0, -16.215855559479365, -43.01594103179767, 0, 0, 0, 0, 0, -10.952073349230062, -16.445555763011413, -16.445555763011413, -16.445555763011413, -16.445555763011413, -22.59699129071464, -22.59699129071464, -26.005503905906323, 0, -30.1506943181019, -30.1506943181019, -33.673773790807786, 0, -33.591902348573, 33.02369931724701, 16.906020670352575, 16.906020670352575, 0, 0, 0, -18.83333666886324, -25.158673490755348, -35.46426430051292, -35.46426430051292, 0, 294.5591855863818, 0, 0, -83.69600586493456, -22.06781578151788, 0, 0   ]\n",
    "#     #pitch = pitch + torch.tensor(pitch_offset).to(\"cuda\")\n",
    "#     print(\"---!\")\n",
    "#     pitch[0][-6] = 180  \n",
    "#     pitch[0][-5] = 260  \n",
    "#     pitch[0][-4] = 360  \n",
    "#     pitch[0][-3] = 360  \n",
    "#     pitch[0][-2] = 380  \n",
    "#     pitch[0][-1] = 400  \n",
    "#     duration = duration*1.3\n",
    "\n",
    "    return pitch ,duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../inference.py {flags3000} -i {fn} -o output/out --pitch-transform-custom  --speaker 0 --n-speakers 2 \n",
    "IPython.display.Audio(\"output/out/audio_0.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "url = \"http://tts.test.vocacloud.net/tts/mel2wav\"\n",
    "reply_fields=['signal','sample_rate','mel_before_denoiser','mel_after_denoiser']\n",
    "\n",
    "text = \"But it involves a happiness that will last throughout our lives, will it not?\"\n",
    "text = \"I felt somehow it would have been an easier job.\"\n",
    "text = \"Notice, no fear. No sense of impending doom. We came on the wrong night.\"\n",
    "train_id = '05322'\n",
    "\n",
    "def show_samples(text):\n",
    "    train_id = sentences[text]\n",
    "\n",
    "    file = get_temp_filename(\"/tmp\",\"temp_\",\".txt\")\n",
    "    with open(file,\"w\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "    wav_name = \"../Jenny/wavs/{}.wav\".format(train_id)\n",
    "    a = wav_name\n",
    "    #print(\"gt_full - {}\".format(wav_name))\n",
    "    #display(Audio(wav_name, normalize=True))\n",
    "\n",
    "    #print(\"api_full\")\n",
    "    out,sr,mel,align, align_text = get_current_jenny(file)\n",
    "    api_file = get_temp_filename(\"/tmp\",\"temp_\",\".wav\")\n",
    "    write(api_file, 22050, out)\n",
    "    b = api_file\n",
    "    #display(Audio(api_file, normalize=True))\n",
    "\n",
    "\n",
    "    # print(\"mel from mels folder. vocoder is Jenny\")\n",
    "\n",
    "    # mel1 = torch.load('../Jenny/mels/{}.pt'.format(train_id))\n",
    "    # mel = mel1\n",
    "    # melFile = '/tmp/mel.pt'\n",
    "    # torch.save(mel,melFile, _use_new_zipfile_serialization=False)\n",
    "    # with open(melFile, 'rb') as fin:\n",
    "    #     files = {'file': fin,'reply_fields': json.dumps(reply_fields)}\n",
    "    #     rawResponse = requests.post(url, files=files, stream=True,verify=False)\n",
    "    # try:\n",
    "    #     reply = torch.load(io.BytesIO(rawResponse.content))\n",
    "    # except Exception as ex:\n",
    "    #     print(rawResponse.content)\n",
    "\n",
    "    # write(\"/tmp/temp222.wav\", reply[\"sample_rate\"], reply[\"signal\"].numpy())\n",
    "    # display(Audio(\"/tmp/temp222.wav\", normalize=True))\n",
    "\n",
    "\n",
    "    # print(\"mel from tacotron. vocoder is Jenny\")\n",
    "\n",
    "    # mel2 = torch.tensor(mel)\n",
    "    # mel = mel2\n",
    "    # melFile = '/tmp/mel.pt'\n",
    "    # torch.save(mel,melFile, _use_new_zipfile_serialization=False)\n",
    "    # with open(melFile, 'rb') as fin:\n",
    "    #     files = {'file': fin,'reply_fields': json.dumps(reply_fields)}\n",
    "    #     rawResponse = requests.post(url, files=files, stream=True,verify=False)\n",
    "    # try:\n",
    "    #     reply = torch.load(io.BytesIO(rawResponse.content))\n",
    "    # except Exception as ex:\n",
    "    #     print(rawResponse.content)\n",
    "\n",
    "    # write(\"/tmp/temp222.wav\", reply[\"sample_rate\"], reply[\"signal\"].numpy())\n",
    "    # display(Audio(\"/tmp/temp222.wav\", normalize=True))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #print(\"mel from fastPitch. vocoder is Universal\")\n",
    "\n",
    "    !python ../inference.py {flags3000} -i {file} -o output/out --pitch-transform-custom  --speaker 0 --n-speakers 2 >/dev/null\n",
    "    c = \"output/out/audio_0.wav\"\n",
    "    #display(Audio(c, normalize=True))\n",
    "\n",
    "\n",
    "\n",
    "    #print(\"mel from fastPitch. vocoder is Jenny\")\n",
    "    d = \"/tmp/MFVJ.wav\"\n",
    "    melFile = 'output/out/audio_0_mel.pt'\n",
    "    with open(melFile, 'rb') as fin:\n",
    "        files = {'file': fin,'reply_fields': json.dumps(reply_fields)}\n",
    "        rawResponse = requests.post(url, files=files, stream=True,verify=False)\n",
    "    try:\n",
    "        reply = torch.load(io.BytesIO(rawResponse.content))\n",
    "    except Exception as ex:\n",
    "        print(rawResponse.content)\n",
    "\n",
    "    write(d, reply[\"sample_rate\"], reply[\"signal\"].numpy())\n",
    "    #display(Audio(d, normalize=True))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    audio_widgets_files  = [a,b,c,d]\n",
    "    audio_widgets = []\n",
    "    for widget_file in audio_widgets_files:\n",
    "        with open(widget_file,\"rb\") as wf:\n",
    "            data = wf.read()\n",
    "        sample_rate = 22050\n",
    "        out = widgets.Output()\n",
    "        with out:\n",
    "            display(Audio(data=data, rate=sample_rate))\n",
    "        audio_widgets.append(out)\n",
    "    display(HBox(audio_widgets))\n",
    "\n",
    "random.seed(15)\n",
    "sentences2 = list(sentences.keys())\n",
    "random.shuffle(sentences2)\n",
    "for s in sentences2[0:15]:\n",
    "    show_samples(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
